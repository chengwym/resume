% !TEX program = xelatex

\documentclass{resume}
%\usepackage{zh_CN-Adobefonts_external} % Simplified Chinese Support using external fonts (./fonts/zh_CN-Adobe/)
%\usepackage{zh_CN-Adobefonts_internal} % Simplified Chinese Support using system fonts

\begin{document}
\pagenumbering{gobble} % suppress displaying page number

\name{Jingbo Cheng}

\basicInfo{
  \email{chengwym@gmail.com} \textperiodcentered\ 
  \phone{(+86) 18686370855} \textperiodcentered\ 
  \github[github]{https://github.com/chengwym} \textperiodcentered\ 
  \linkedin[linkedin]{https://www.linkedin.com/in/jingbo-cheng-a661a3252}}

\section{\faGraduationCap\ Education}
\datedsubsection{\textbf{Peking University (PKU)}}{2021 -- 2025}
\textit{B.S.} in Computer Science. GPA 3.7

\section{\faHeartO\ Honors and Awards}
\datedline{\textit{Silver Medal in the 37th Chinese Physics Olympiad (CPhO)}}{Oct. 2020}


\section{\faUsers\ Internship}
\datedsubsection{\textbf{Probquant } \textit{Machine Learning intern}}{Dec. 2022 -- Present}
Modular Automated Training Framework
\begin{itemize}
  \item To build a complete training framework, the experiment is divided into feature selection, model training, and prediction result generation report. The core file is a configuration file, and you only need to modify some parameters in the configuration file to compare results in batches.
  \item Use numba, CPython and other acceleration methods to improve the backtest time.
\end{itemize}
Model Fusion
\begin{itemize}
  \item In a low-frequency trading scenario, different values of y were used with different loss functions to train multiple models, and the resulting signals were combined using various methods. The signals were then fused using XGBoost, which increased the interpretability of the signals. After deducting fees, the fused signals produced an index-enhancing annualized excess return of 42\%, which is a significant improvement compared to the individual signal results prior to fusion.
\end{itemize}

\datedsubsection{\textbf{Probquant } \textit{Machine Learning intern}}{Dec. 2022 -- Present}
Modular Automated Training Framework
\begin{itemize}
  \item To build a complete training framework, the experiment is divided into feature selection, model training, and prediction result generation report. The core file is a configuration file, and you only need to modify some parameters in the configuration file to compare results in batches.
  \item Use numba, CPython and other acceleration methods to improve the backtest time.
\end{itemize}
Model Fusion
\begin{itemize}
  \item In a low-frequency trading scenario, different values of y were used with different loss functions to train multiple models, and the resulting signals were combined using various methods. The signals were then fused using XGBoost, which increased the interpretability of the signals. After deducting fees, the fused signals produced an index-enhancing annualized excess return of 42\%, which is a significant improvement compared to the individual signal results prior to fusion.
\end{itemize}

\section{\faObjectGroup\ Projects}
\datedsubsection{\textbf{Using Computer Vision to Study Seismic Waveforms } \textit{PKU}}{Dec. 2022 -- Jan. 2023}
\begin{itemize}
  \item Process the data in mseed format to obtain a seismogram with offset as the abscissa and time as the ordinate.
  Classification of seismic atlases with resnet models.
  \item Use the resnet model to predict the focal depth of the earthquake and the azimuth of the glacier collapse through the seismic map. and the latitude and longitude of the place of origin.
  \item Using deep-cluster, unsupervised image classification, classify different types of earthquakes.
\end{itemize}

\datedsubsection{\textbf{Kaggle predicts credit card score }\textit{PKU}}{May. 2022 -- Jun. 2022}
\begin{itemize}
  \item The credit card scoring model based on gradient boosting decision tree, based on the Kaggle public data set, has carried out fine feature engineering and strict statistical analysis.
  For the analysis, the gradient boosting decision tree is selected as the algorithm, and an economically meaningful model is established to convert credit points.
  \item Select the gradient boosting decision tree model, use Baysian Optimization for hyperparameter optimization, cross-validate to select the number of training rounds, and then formally train the model
  Type, calculate the importance of features - calculated according to the splitting of the nodes of the decision tree.
  \item In the prediction of default probability, an excellent AUC of 0.86910 was obtained, which is the same score as the top five in the list.
\end{itemize}

\section{\faCogs\ Skills}
\begin{itemize}[parsep=0.5ex]
  \item C++, Python, PyTorch, Linux, LaTeX, Shell Script, HTML, JavaScript, Java
  \item CET4, CET6
\end{itemize}
%% Reference
%\newpage
%\bibliographystyle{IEEETran}
%\bibliography{mycite}

\end{document}
